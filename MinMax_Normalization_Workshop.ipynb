{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè° Min-Max Normalization Workshop\n",
    "## Team Name: FIVE\n",
    "## Team Members: Fasalu Rahman Kottaparambu, Christo Pananjickal Baby\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó Why We Normalize: The Problem with Raw Feature Scales\n",
    "\n",
    "In housing data, features like `Price` and `Lot_Size` can have values in the hundreds of thousands, while others like `Num_Bedrooms` range from 1 to 5. This creates problems when we use algorithms that depend on numeric magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è What Goes Wrong Without Normalization\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üß≠ K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN uses the **Euclidean distance** formula:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + \\cdots}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- $ \\text{Price}_1 = 650{,}000, \\quad \\text{Price}_2 = 250{,}000 $\n",
    "- $ \\text{Bedrooms}_1 = 3, \\quad \\text{Bedrooms}_2 = 2 $\n",
    "\n",
    "Now compute squared differences:\n",
    "\n",
    "$$\n",
    "(\\text{Price}_1 - \\text{Price}_2)^2 = (650{,}000 - 250{,}000)^2 = (400{,}000)^2 = 1.6 \\times 10^{11}\n",
    "$$\n",
    "$$\n",
    "(\\text{Bedrooms}_1 - \\text{Bedrooms}_2)^2 = (3 - 2)^2 = 1\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è **Price dominates the distance calculation**, making smaller features like `Bedrooms` irrelevant.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üìâ Linear Regression\n",
    "\n",
    "Linear regression estimates:\n",
    "\n",
    "$$\n",
    "y = \\beta_1 \\cdot \\text{Price} + \\beta_2 \\cdot \\text{Bedrooms} + \\beta_3 \\cdot \\text{Lot\\_Size} + \\epsilon\n",
    "$$\n",
    "\n",
    "If `Price` has very large values:\n",
    "- Gradient updates for $ \\beta_1 $ will be **much larger**\n",
    "- Gradient updates for $ \\beta_2 $ (Bedrooms) will be **very small**\n",
    "\n",
    "‚û°Ô∏è The model overfits high-magnitude features like `Price`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üß† Neural Networks\n",
    "\n",
    "A single neuron computes:\n",
    "\n",
    "$$\n",
    "z = w_1 \\cdot \\text{Price} + w_2 \\cdot \\text{Bedrooms} + w_3 \\cdot \\text{Lot\\_Size}\n",
    "$$\n",
    "\n",
    "If:\n",
    "\n",
    "- $ \\text{Price} = 650{,}000 $\n",
    "- $ \\text{Bedrooms} = 3 $\n",
    "- $ \\text{Lot\\_Size} = 8{,}000 $\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "z \\approx w_1 \\cdot 650{,}000 + w_2 \\cdot 3 + w_3 \\cdot 8{,}000\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è Even with equal weights, `Price` contributes **most of the activation**, making it difficult for the network to learn from other features.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution: Min-Max Normalization\n",
    "\n",
    "We apply the transformation:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "This scales all features to a common range (typically $[0, 1]$).\n",
    "\n",
    "| Feature      | Raw Value | Min     | Max     | Normalized Value |\n",
    "|--------------|-----------|---------|---------|------------------|\n",
    "| Price        | 650,000   | 250,000 | 800,000 | 0.72             |\n",
    "| Bedrooms     | 3         | 1       | 5       | 0.50             |\n",
    "| Lot_Size     | 8,000     | 3,000   | 10,000  | 0.714            |\n",
    "\n",
    "‚û°Ô∏è Now, **each feature contributes fairly** to model training or distance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Use Case: Housing Data\n",
    "We are normalizing features from a real estate dataset to prepare it for machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:38:56.676684Z",
     "start_time": "2025-06-18T15:38:56.667546Z"
    }
   },
   "source": [
    "# üî¢ Load and display dataset\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "df = pd.read_csv('data/housing_data.csv')\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='psql',showindex=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+----------------+-----------------+--------------+------------+\n",
      "| House_ID   |   Price |   Area_sqft |   Num_Bedrooms |   Num_Bathrooms |   Year_Built |   Lot_Size |\n",
      "|------------+---------+-------------+----------------+-----------------+--------------+------------|\n",
      "| H100000    |  574507 |        1462 |              3 |               3 |         2002 |       4878 |\n",
      "| H100001    |  479260 |        1727 |              2 |               2 |         1979 |       4943 |\n",
      "| H100002    |  597153 |        1403 |              5 |               2 |         1952 |       5595 |\n",
      "| H100003    |  728454 |        1646 |              5 |               2 |         1992 |       9305 |\n",
      "| H100004    |  464876 |         853 |              1 |               1 |         1956 |       7407 |\n",
      "+------------+---------+-------------+----------------+-----------------+--------------+------------+\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Step 1 ‚Äî Implement Min-Max Normalization on the Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:38:49.065605Z",
     "start_time": "2025-06-18T15:38:49.050794Z"
    }
   },
   "source": [
    "# ‚úçÔ∏è Implement Min-Max Normalization manually here (no sklearn/numpy)\n",
    "# Normalize: Price, Area_sqft, Num_Bedrooms, Num_Bathrooms, Lot_Size\n",
    "\n",
    "\n",
    "# Drop unnecessary columns (House_ID)\n",
    "df = df.drop(columns=['House_ID'])\n",
    "\n",
    "\n",
    "# Clean the dataset by filling missing values with median\n",
    "columns_to_clean = ['Price', 'Area_sqft', 'Num_Bedrooms', 'Num_Bathrooms', 'Year_Built', 'Lot_Size']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    median_value = df[col].median()\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='psql',showindex=False))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------------+-----------------+--------------+------------+\n",
      "|   Price |   Area_sqft |   Num_Bedrooms |   Num_Bathrooms |   Year_Built |   Lot_Size |\n",
      "|---------+-------------+----------------+-----------------+--------------+------------|\n",
      "|  574507 |        1462 |              3 |               3 |         2002 |       4878 |\n",
      "|  479260 |        1727 |              2 |               2 |         1979 |       4943 |\n",
      "|  597153 |        1403 |              5 |               2 |         1952 |       5595 |\n",
      "|  728454 |        1646 |              5 |               2 |         1992 |       9305 |\n",
      "|  464876 |         853 |              1 |               1 |         1956 |       7407 |\n",
      "+---------+-------------+----------------+-----------------+--------------+------------+\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Talking Point 1 ‚Äî [Insert your review comment here]\n",
    "\n",
    "Reviwed by:\n",
    "- Name\n",
    "- Name\n",
    "- Name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
